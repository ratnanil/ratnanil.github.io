[
  {
    "objectID": "blog/2021-11-05-warping-switzerland-back-into-shape/stormy-transformations.html",
    "href": "blog/2021-11-05-warping-switzerland-back-into-shape/stormy-transformations.html",
    "title": "Warping Switzerland back into shape",
    "section": "",
    "text": "When I started working with swiss geodata a couple of years ago, Switzerland was beginning to transition from it’s old coordinate system CH1903 LV03 to CH1903+ LV95 (EPSG 2056).\nThe new coordinate system was awkward from the start. First off, I suddenly was un unable to convert data from WGS84 using a custom R script that swisstopo had previously provided for CH1903 LV03. The answer to my question on a swisstopo google group (in 2017) was to use swisstopo’s REST api.\nThis problem was seemingly solved with the emergence of sf and it’s built-in methods to reproject coordinates easily. Until one day I realized that the coordinate transformations to and from EPSG 2056 were fairly imprecise. The transformations did not account for the spatially varying offset for which the new coordinate system was implemented in the first place.\nI filed an issue on sf’s github repo beginning of 2020, but it wasn’t until recently (with PROJ 7.0.0) that I was able to precisely transform my data to and from EPSG 2056.\nThe issue is highlighted with the function list_coordOps() from rgdal. Apparently, the transformation was lacking a grid with the name ch_swisstopo_CHENyx06a.tif (the situation is now different, see my edit below).\nWith my current version of sf and rgdal, I need to download this grid manually and move it to the PROJ directory. Running list_coordOps now shows a different output.\nThe downloaded file is a so called transformation grid: A raster dataset in WGS84 containing information on the lat and lon offset for a given cell.\nI now can use this grid to precisely transform coordinates to and from ESPG 2056. I will demonstrate this by visualizing the directional offset, similar to the transformation grid I downloaded."
  },
  {
    "objectID": "blog/2021-11-05-warping-switzerland-back-into-shape/stormy-transformations.html#edit-2022-04-24",
    "href": "blog/2021-11-05-warping-switzerland-back-into-shape/stormy-transformations.html#edit-2022-04-24",
    "title": "Warping Switzerland back into shape",
    "section": "EDIT (2022-04-24):",
    "text": "EDIT (2022-04-24):\nAt the time of writing this blogpost, the bash command to download and move the file to the correct place was as follows:\nwget https://cdn.proj.org/ch_swisstopo_CHENyx06a.tif`\nsudo mv ch_swisstopo_CHENyx06a.tif /usr/share/proj/\nToday however, I needed to download a zip file and unzip it to /usr/share/proj/. I assume this will change again in the near future.\nhttps://download.osgeo.org/proj/proj-datumgrid-europe-1.5.zip \nsudo unzip proj-datumgrid-europe-1.5 /usr/share/proj/"
  },
  {
    "objectID": "blog/2021-10-22-benchmarking-binary-predicates/benchmarking_binary_predicates.html",
    "href": "blog/2021-10-22-benchmarking-binary-predicates/benchmarking_binary_predicates.html",
    "title": "Benchmarking binary predicates",
    "section": "",
    "text": "I often work with geodata in R and come across situations where I need to subset points based on whether they lie within a polygon or not. There are several functions to solve this problem1. From the package sf, the functions st_within, st_contains, st_intersects and st_covered_by can all answer this question2. I noticed that with big datasets, some of these functions are unbearably slow. To find out which one is faster in which scenario, I decided to benchmark these four functions.\nTo make things more interesting, I won’t use my usual Swiss data for this test, but data from my second home, Sri Lanka. More specifically: I will use the Geonames data (> 50k points) and the administrative boundaries of Sri Lanka (26 polygons).\n\n\nloading libraries\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(sf)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(microbenchmark)\nlibrary(ggridges)\nlibrary(forcats)\n\n\n\n\npreparing boundary data\n# Downloaded from: https://data.humdata.org/dataset/sri-lanka-administrative-levels-0-4-boundaries\n# Administrative Level 0: country (1 features)\n# Administrative Level 1: province (9 features)\n# Administrative Level 2: district (26 features)\n# Administrative Level 3: divisional secretatiat (333 features)\n# Administrative Level 4: grama niladhari (14'044 features)\n\ntmp <- tempdir()\n\nboundary_dir <- file.path(tmp, \"boundary\")\nunzip(\"data-git-lfs/lka_adm_slsd_20200305_shp.zip\", exdir = boundary_dir)\n\nsl_boundary_l2 <- read_sf(\n  file.path(boundary_dir, \"lka_admbnda_adm2_slsd_20200305.shp\")\n  )\n# https://epsg.io/5234\n# https://epsg.io/5235\n\n\n\n\npreparing geonames datase\n# geonameid         : integer id of record in geonames database\n# name              : name of geographical point (utf8) varchar(200)\n# asciiname         : name of geographical point in plain ascii characters, varchar(200)\n# alternatenames    : alternatenames, comma separated, ascii names automatically transliterated, convenience attribute from alternatename table, varchar(10000)\n# latitude          : latitude in decimal degrees (wgs84)\n# longitude         : longitude in decimal degrees (wgs84)\n# feature class     : see http://www.geonames.org/export/codes.html, char(1)\n# feature code      : see http://www.geonames.org/export/codes.html, varchar(10)\n# country code      : ISO-3166 2-letter country code, 2 characters\n# cc2               : alternate country codes, comma separated, ISO-3166 2-letter country code, 200 characters\n# admin1 code       : fipscode (subject to change to iso code), see exceptions below, see file admin1Codes.txt for display names of this code; varchar(20)\n# admin2 code       : code for the second administrative division, a county in the US, see file admin2Codes.txt; varchar(80) \n# admin3 code       : code for third level administrative division, varchar(20)\n# admin4 code       : code for fourth level administrative division, varchar(20)\n# population        : bigint (8 byte int) \n# elevation         : in meters, integer\n# dem               : digital elevation model, srtm3 or gtopo30, average elevation of 3''x3'' (ca 90mx90m) or 30''x30'' (ca 900mx900m) area in meters, integer. srtm processed by cgiar/ciat.\n# timezone          : the iana timezone id (see file timeZone.txt) varchar(40)\n# modification date : date of last modification in yyyy-MM-dd format\n\n\ncolnames <- c(\"geonameid\", \"name\",  \"asciiname\",  \"alternatenames\", \"latitude\",  \n              \"longitude\",  \"feature_class\",  \"feature_code\", \"country_code\",  \n              \"cc2\",  \"admin1_code\",  \"admin2_code\",  \"admin3_code\",  \n              \"admin4_code\",  \"population\", \"elevation\", \"dem\", \"timezone\",  \n              \"modification_date\")\n\n\ngeonames_dir <- file.path(tmp, \"geonames\")\n\nunzip(\"data-git-lfs/LK.zip\", exdir = geonames_dir)\n\ngeonames <- read_tsv(file.path(geonames_dir, \"LK.txt\"),col_names = colnames) %>%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nRows: 56748 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr   (8): name, asciiname, alternatenames, feature_class, feature_code, cou...\ndbl  (10): geonameid, latitude, longitude, admin1_code, admin2_code, admin3_...\ndate  (1): modification_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOnce all the data is imported, I can demonstrate visually the task. I want to subset all points within the province of Kandy (incidentally where I spent 5 superb years of my childhood). Using st_within() for this operation, the output looks like this:\n\n\n\nsubsetting and creating a map\nkandy <- filter(sl_boundary_l2, ADM2_EN == \"Kandy\")\n\npoints_filter <- list(\n  within = geonames[st_within(geonames,kandy,sparse = FALSE)[,1],]\n)\n\nplot_bg_col <- Sys.getenv(\"plot_bg_col\")\ntext_col <- Sys.getenv(\"text_col\")\n\np1 <- ggplot(sl_boundary_l2) + \n  geom_sf(color = \"#ffffff\", fill = \"#ababab\") +\n  geom_sf(data = rbind(transmute(geonames, val = \"all points\"), \n                       transmute(points_filter[[\"within\"]], \n                                 val = \"points within the\\nprovince of Kandy\")), \n          alpha = 0.05, size = 0.05, color = \"#8d2663\") +\n  geom_sf(data = ~filter(., ADM2_EN == \"Kandy\"), fill = NA, color = \"#000000\") +\n  facet_wrap(~val) +\n  coord_sf(xlim = c(78, 83)) + \n  theme(strip.background = element_blank(),\n        strip.text = element_text(color = text_col),\n        panel.background = element_blank(),\n        plot.background = element_rect(fill = plot_bg_col),\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        )\n\np1\n\n\n\n\n\nNext, I will do the same operation with the other functions and also check the output number of rows to see if they are similar (they might be slightly off if we have points exactly on the polygon boundary) or even identical.\n\npoints_filter[[\"contains\"]] <- geonames[st_contains(kandy,\n                                                    geonames,\n                                                    sparse = FALSE)[1,],]\npoints_filter[[\"intersects\"]] <- geonames[st_intersects(geonames,\n                                                        kandy,\n                                                        sparse = FALSE)[,1],]\npoints_filter[[\"covered_by\"]] <- geonames[st_covered_by(geonames,\n                                                        kandy,\n                                                        sparse = FALSE)[,1],]\n\ntibble(\n  function_name = names(points_filter),\n  nrow = sapply(points_filter, nrow),\n  identical_to_st_within = sapply(points_filter, function(x){\n    identical(points_filter[[\"within\"]], x)\n    })\n) %>%\n  knitr::kable(col.names = stringr::str_replace_all(colnames(.),\"_\", \" \"))\n\n\n\n\nfunction name\nnrow\nidentical to st within\n\n\n\n\nwithin\n3251\nTRUE\n\n\ncontains\n3251\nTRUE\n\n\nintersects\n3251\nTRUE\n\n\ncovered_by\n3251\nTRUE\n\n\n\n\n\nTo find out which function is the fastest, I use the package microbenchmark. Since it doesn’t always take the same amount of time to process the same function, each function is executed multiple times (times = 50) and we will look at the distribution of the execution times.\n\n\nBenchmarking the functions\nmbm  <- microbenchmark(\n  intersects = st_intersects(kandy,geonames),\n  within = st_within(geonames,kandy),\n  contains = st_contains(kandy,geonames),\n  covered_by = st_covered_by(geonames,kandy),\n  times = 50\n)\n\n\n\n\n\n\n\n\n\n\nvisualizing the result\nmbm2df <- function(mbm_obj){\n  df <- as.data.frame(mbm_obj)\n  df$time <- dnanoseconds(df$time)\n  df\n}\n\nmbm_df <- mbm2df(mbm)\n\n\np2 <- mbm_df %>%\n  mutate(\n    expr = fct_reorder(expr,time,median,.desc = TRUE)\n  ) %>%\n  ggplot(aes(time,expr,fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +\n  scale_fill_viridis_c(option = \"C\")  +\n  scale_x_time(name = \"Duration (in seconds)\",\n               labels = scales::time_format(format = \"%OS3\")) +\n  labs(y = \"Function\") +\n  theme_minimal() +\n  theme(legend.position=\"none\",\n        plot.background = element_rect(fill = plot_bg_col), panel.grid.minor.x  = element_blank(),panel.grid.major.x = element_blank(),  axis.text = element_text(colour = text_col), text = element_text(colour = text_col)) \n\np2\n\n\nPicking joint bandwidth of 0.0204\n\n\n\n\n\nThis benchmark shows that st_contains and st_intersects executes faster than st_covered_by and st_within. The next question is: How do the functions scale and perform under different scenarios? I’ll test this by generating additional points to subset, and also by using more provinces than just Kandy. And since I’m more interested in relative rather than absolute execution times, I will calculate the median duration per function and scenario and rescale the values by deviding them with the duration of st_intersects.\n\n\nBenchmarking scalability\nn_points_vec <- c(100e3,200e3,500e3)\nn_poly_vec <- c(1,9,17,26)\n\nmbm2 <- map_dfr(n_points_vec,function(n_points){\n  \n  points <- st_sample(sl_boundary_l2,n_points,what = \"centers\")\n\n  mbm_points <- map_dfr(n_poly_vec, function(n_poly){\n    \n    polygons <- sample_n(sl_boundary_l2, n_poly)\n    \n    mbm_poly <- microbenchmark(\n      intersects = st_intersects(polygons,points),\n      within = st_within(points,polygons),\n      contains = st_contains(polygons,points),\n      covered_by = st_covered_by(points,polygons),\n      times = 10\n      )\n\n    as_tibble(mbm_poly) %>%\n      mutate(n_poly = n_poly)\n  }) %>%\n    mutate(n_points = n_points)\n})\n\n\n\n\n\n\n\n\n\n\nVisualizing results\nmbm2_df <- mbm2df(mbm2)\n\n\n\nmylabels <- function(x){sprintf(\"%+3.f%%\", x*100)}\n\nmbm2_df %>%\n  group_by(expr, n_poly, n_points) %>%\n  summarise(median = median(time)) %>% \n  ungroup() %>%\n  group_by(n_poly,n_points) %>%\n  mutate(\n    perc = median/median[expr == \"contains\"]-1,\n    expr = fct_relevel(expr, \"within\", \"covered_by\",\"intersects\", \"contains\")\n    ) %>%\n  ggplot(aes(perc,as.factor(expr), color = expr, fill = expr)) +\n  geom_point() +\n  geom_linerange(aes(xmin = 0, xmax = perc)) +\n  # expand_limits(x = 0) +\n  scale_x_continuous(\"Relative execution time (compared to 'st_contains')\", \n                     breaks = seq(-.4,.4,.2), labels = mylabels,\n                     limits = c(-.5,.5), \n                     sec.axis = sec_axis(~.x, \n                                         breaks = c(-.4,.4), \n                                         labels = c(\"< faster\",\"slower > \"))) +\n  labs(y = \"\") +\n  facet_grid(n_poly~n_points, \n             labeller = labeller(n_points = ~paste0(as.integer(.x)/1e3, \"K points\"),\n                                 n_poly = ~paste0(.x, \" polygons\")))+\n  # theme_light() +\n  theme(legend.position = \"none\", \n        axis.ticks.x.top = element_blank(), \n        text = element_text(size = 9),\n        plot.background = element_rect(fill = plot_bg_col),\n        panel.background = element_rect(fill = plot_bg_col),\n        panel.grid = element_blank(),axis.text = element_text(colour = text_col),axis.title = element_text(colour = text_col),strip.background = element_rect(fill = text_col)\n        )\n\n\n\n\n\n\n\nvisualizing the result\nmbm2_df %>%\n  group_by(expr, n_poly, n_points) %>%\n  summarise(median = median(time)) %>% \n  ungroup() %>%\n  ggplot(aes(n_poly,median, color = expr, fill = expr)) +\n  geom_point() +\n  geom_line()  +\n  scale_y_time(name = \"Duration (in seconds)\", \n               labels = scales::time_format(format = \"%OS2\")) +\n  scale_x_continuous(name = \"Number of Polygons\") +\n  facet_wrap(~n_points, \n             labeller = labeller(n_points = ~paste0(as.integer(.x)/1e3, \"K points\")), \n             scales = \"free_y\", ncol = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nThis test shows something interesting: While st_contains and st_intersects are fast with a single polygon, they don’t scale well with lager number of polygons. This effect is especially prominent when the number of points is large (~500K).\nMy take home message from this whole exercise: If you want to subset points based on whether they lie in specific polygons or not, use st_intersects or st_contains when the number of polygons is small. Use st_covered_by or st_within when the number of polygons is large. If it’s important what happens to points lying on the edge, remember that only st_intersects and st_covered_by will include them.\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nespecially since I mostly don’t care what happens to point which lie exactly on the polygon edge↩︎\nst_within and st_contains will disregard points on a line, st_intersects and st_covered_by will include them↩︎"
  },
  {
    "objectID": "blog/2022-11-30-ArcGIS-vs-QGIS/index.html",
    "href": "blog/2022-11-30-ArcGIS-vs-QGIS/index.html",
    "title": "Teaching geospatial: ArcGIS or QGIS?",
    "section": "",
    "text": "Disclaimer\n\n\n\nThe opinions expressed in this post are my own and do not necessarily reflect those of my employer.\n\n\nArcGIS was considered to be the “industry standard” for a long time. Therefore, our Uni started out using ESRI products when initially developing the first geospatial courses (I’m trying hard to omit the term GIS). Since ESRI has implemented an attractive pricing model for universities and students, there is a low entry barrier to using their products in academia. However, using a commercial product has several downsides: Once students are done with their studies, they will pay the debt for the years of using ArcGIS for free. And I do not mean this strictly monetarily (even though the license fees are considerable): The proprietary nature of the software makes them dependent on ESRI for all updates, support, and fixes. The suite of ESRI products that are exclusively interoperable makes vendor lock-in a big issue, they become less flexible.\nOn the other hand, using Free and OpenSource Software (FOSS) like QGIS has number of advantages: QGIS is maintained by the community, so bugs are usually discovered and fixed very quickly (if they aren’t one can hire a maintainer to do so). Similarly, new features can be implemented very easily, e.g. by creating a QGIS Plugin. QGIS invites tinkering with the software and peeking behind the scenes. There is a lot that speaks for using FOSS in general, and in an academic setting specifically (see reproducibility). I won’t go further into these reasons here; this blogpost is about something else. Instead, I will refer you to the articles written by Richard Stallman and the Free Software Foundation on https://www.gnu.org/education/\n\n\n\n\n\n\n\n\n\n\nMy favorite software for geospatial are R (sf, terra, stars) and Python (geopandas, rasterio, xarray), which we teach on a bachelor and master level. Depending on the audience, these script based tools can be overwhelming and the wrong tool for the job. In the last couple of years, we have started implementing QGIS in new courses or when overhauling existing ones. From the feedback I get from our students who used both software, they prefer working with QGIS. The main reasons being that in their view, QGIS runs more stable and they enjoy the prospect that they can use it after they’ve completed their studies.\nArcGIS vs. QGIS is an ongoing debate on social media. In this discussion, many people make the valid point that you don’t have to choose and can use both software. Another point often made is that you should teach concepts, not software. I fully agree with both statements, however not choosing a specific software is a bit impractical in an education setting. In this blogpost I will therefore continue to focus on the ArcGIS vs. QGIS debate.\n\n\n\n\n\n\n\n\n\n\nThe big pro argument for using ArcGIS seems to be ArcGIS is the industry standard, therfore the students need to learn ArcGIS. This seems to be a classical chicken-egg problem: Do we teach software \\(X\\) because it’s the industry standard, or is software \\(X\\) the industry standard because it’s what we teach? Be it as it may, I’ve started digging into this premise of this argument to find data to compare the popularity of ArcGIS vs. QGIS. So far, I’ve only found two datasets on this topic (if you have more, please get in touch (rata@zhaw.ch) or write into the comment section below).\n\nThe tags from questions on gis.stackexchange.com (downloaded here on the 17.11.2022)\nGoogle Trends search data (downloaded here on the 17.11.2022)\n\nI’ve downloaded, processed and visualized this data with R. You can display the code for these steps below, or just look at the results.\n\n\nLoading necessary libraries\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lubridate)\n#install.packages(\"ggfx\")\nlibrary(ggfx)\nlibrary(ggtext)\nlibrary(ragg)\nlibrary(colorspace)\n\n\nThe website gis.stackexchange.com is a community within the Stackexchange network, where questions surrounding GIS can be placed. Each question can be attributed with up to 5 tags to categorize the question and make it easier to find. To compare ArcGIS and QGIS, I counted the number of times a variant of the tag ArcGIS was used compared to the number of times a variant of the tag QGIS was used. There are 77 different tags containing ArcGIS, including [arcgis-desktop], [arcgis-10.0], [arcgis-pro] and so on. Since QGIS has less products, there are only 50 different tags containing QGIS, including [qgis], [pyqgis], [qgis-3] and others.\n\n\nExtract tags from xml\nlibrary(tidyverse)\nlibrary(xml2)\n\n# https://archive.org/details/stackexchange\nxml_posts <- xml2::read_xml(\"blog/2022-11-30-ArcGIS-vs-QGIS/posts/Posts.xml\") \n\n# https://stackoverflow.com/questions/1390568/how-can-i-match-on-an-attribute-that-contains-a-certain-string\n\nget_questions <- function(tags, xml_posts, attrs = c(\"Id\", \"Tags\", \"AnswerCount\",\"AcceptedAnswerId\", \"PostTypeId\", \"CreationDate\", \"ViewCount\", \"Score\")){\n  imap_dfr(tags, function(tag, group){\n    children_filter <- xml2::xml_find_all(xml_posts, glue::glue(\"//row[contains(@Tags, '{tag}')]\"))\n    \n    out_df <- map(attrs, function(attr_i){\n      xml2::xml_attr(children_filter,attr = attr_i)\n    }) %>%\n      set_names(attrs) %>%\n      do.call(cbind, .) %>% \n      as_tibble() %>%\n      mutate(\n        searchtag = tag,\n        CreationDate = parse_datetime(CreationDate),\n        AnswerCount = as.integer(AnswerCount),\n        AcceptedAnswerId = as.integer(AcceptedAnswerId),\n        PostTypeId = as.integer(PostTypeId),\n        group = group\n      )\n  })\n  \n}\n\narcgis_qgis <- get_questions(c(arcgis = \"arcgis\", arcgis = \"arcpy\", qgis = \"qgis\"),xml_posts)\n\nwrite_csv(arcgis_qgis, \"blog/2022-11-30-ArcGIS-vs-QGIS/gis.stackexchange-tags.csv\")\n\n\n\n\nPreparing tags\ngis_stackexchange_tags <- read_csv(here(\"blog\",\"2022-11-30-ArcGIS-vs-QGIS\", \"gis.stackexchange-tags.csv\"))\n\narcgis_pa <- gis_stackexchange_tags |>\n  mutate(year = year(CreationDate)) |>\n  filter(year < 2022) |>\n  group_by(year, group) |>\n  count()\n\narcgis_qgis_roll <- gis_stackexchange_tags |>\n  mutate(month = as.Date(lubridate::round_date(CreationDate, \"month\"))) |> \n  group_by(group, month) |>\n  count() |>\n  group_by(group) |>\n  mutate(roll = zoo::rollmean(n, 12, align = \"left\", fill = NA)) |>\n  filter(!is.na(roll)) |>\n  ungroup() |>\n  complete(group, month, fill = list(n = 0, roll = 0))\n\n\n\n\narcgis_qgis_wide <- arcgis_qgis_roll |>\n  group_by(month) |>\n  summarise(\n    min = min(roll),\n    max = max(roll),\n    diff = max - min,\n    group = group[which.max(roll)]\n  )\n\n\n# https://coolors.co/palette/ef476f-ffd166-06d6a0-118ab2-073b4c\ncols <- c(arcgis = \"#06d6a0\", qgis = \"#ef476f\")\nbgcol <- \"#073b4c\"\n\n\nget_first <- function(vec, comp){\n  comps <- vec >= comp\n  if(!any(comps)){\n    length(comps)\n  }else{\n    min(which(comps))\n  }\n}\n\nvlines <- tibble(y =  seq(0, 600, 100), xend = arcgis_qgis_roll$month[map_int(y, ~get_first(arcgis_qgis_roll$roll, .x))], x = rep(min(arcgis_qgis_roll$month),length(y)))\n\n\nTo take out some of the noise, I smoothed the monthly counts by taking the mean over a moving window of 1 year. The visualization technique is heavily inspired by Cedric Scherer’s 2021 #30DayChartChallenge (Day 4).\n\n\nVisualizing data from gis.stackexchange.com\nggplot(arcgis_qgis_roll, aes(month, roll, color = group)) +\n  geom_ribbon(\n    data = arcgis_qgis_wide,\n    aes(x = month, ymin = min, ymax = max, color = group,\n        fill = after_scale(darken(desaturate(color, .1), .4, space = \"HLS\"))),\n    alpha = .7, inherit.aes = FALSE\n  ) +\n  with_blur(\n    geom_line(data = ~filter(., group == \"arcgis\"), \n              color = cols[1], size = 2.5),\n    colour = lighten(cols[1], .1), sigma = 3\n  ) +\n  with_blur(\n    geom_line(data = ~filter(., group == \"qgis\"), \n              color = cols[2], size = 2.5),\n    colour = lighten(cols[2], .1), sigma = 3\n  )  +\n  geom_line(size = 1.5) +\n  geom_richtext(\n    aes(x = as.Date(\"2015-06-01\"), y = 700,\n        label = glue::glue(\"Monthly <span style='font-family:cinzel;'><b style='color:{cols['arcgis']};font-size:30pt;'>[ArcGIS]</b> & <b style='color:{cols['qgis']};font-size:30pt;'>[QGIS]</b></span> Questions on gis.stackoverflow.com\")),\n    color = \"grey80\", size = 7, lineheight = 1.5, family = \"Work Sans\",\n    stat = \"unique\", fill = NA, label.color = NA\n  )  +\n  annotate(\"text\", x = as.Date(\"2015-01-27\"), y = 620,vjust = .5, hjust = .5,\n             label = \"ESRI realeases\\nArcGIS Pro 1.0\",\n             family = \"Chivo\",\n           colour = cols[\"arcgis\"],\n             size = 3.8,\n             lineheight = .9) +\n  geom_segment(aes(x = as.Date(\"2015-01-27\"), xend = as.Date(\"2015-01-27\"), \n                   y = 530, yend = 600),colour = cols[\"arcgis\"]) +\n  annotate(\"text\", x = as.Date(\"2016-10-15\"), y = 620,vjust = .5, hjust = .5,\n             label = \"No. of QGIS Tags\\nsurpasses ArcGIS\",\n             family = \"Chivo\",\n           colour = cols[\"qgis\"],\n             size = 3.8,\n             lineheight = .9) +\n  geom_segment(aes(x = as.Date(\"2016-10-15\"), xend = as.Date(\"2016-10-15\"), \n                   y = 430, yend = 600),colour = cols[\"qgis\"]) +\n  geom_segment(data = vlines, aes(x = x, xend = xend, y = y, yend = y), inherit.aes = FALSE,  color = \"grey50\", linetype = \"13\") +\n  geom_segment(data = filter(arcgis_qgis_wide, month(month) == 1), aes(x = month, xend = month, y = 0, yend = min-10), color = \"grey50\", linetype = \"13\") +\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\", expand = c(0, 0))  +\n  scale_y_continuous(breaks = vlines$y) +\n  scale_color_manual(values = cols) +\n  labs(caption = \"Data from gis.stackexchange.com | Visualisation by Nils Ratnaweera | ratnaweera.xyz\") +\n  theme_void() +\n  theme(\n    panel.background = element_rect(fill = \"transparent\", color = \"transparent\"),\n    plot.background = element_rect(fill = bgcol),\n    axis.text = element_text(family = \"Cinzel\", color = \"grey80\",\n                               size = 10, face = \"bold\", margin = margin(t = 6)),\n    panel.grid.major.x = element_blank(),\n    plot.margin = margin(15, 30, 10, 30),\n    plot.caption = element_text(family = \"Work Sans\", color = \"grey50\", size = 8,\n                                hjust = .5, margin = margin(t = 30, b = 0)),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nThe data from gis.stackexchange shows that both ArcGIS and QGIS have grown in popularity since 2011. ArcGIS hit a peak around 2014, 2015 with the release of ArcGIS pro and has since been on a decline. The number of QGIS questions has overtaken those of ArcGIS towards the end of 2016 and QGIS has been dominating the portal ever since.\nThis data seems to clearly show that QGIS is more popular than ArcGIS. However, an argument can be made that the number of questions regarding a program does not reflect it’s popularity. On the contrary, more questions could be a sign of ambiguity: QGIS might provoke more questions in its use, while the use of ArcGIS is more straightforward. Or maybe, questions regarding ArcGIS are not posted on gis.stackexchange.com, but on a different platform, e.g. community.esri.com.\nTherefore, I’ve additionally looked at a different data source, namely Google Trends. Google Trends provides access to a sample of search requests made to Google. This allows us to analyze interest in a particular topic worldwide or for a specific country. Google Trends normalizes search data from 0 to 100 to make comparisons between terms easier. Each data point is divided by the total searches in the selected geographic extent and time range to compare relative popularity.\nI downloaded these normalized values for the search terms ArcGIS and QGIS for the maximum time range (2004 to current) in the geographic extent Switzerland. As before, I smoothed the monthly values with a moving window of 1 year.\n\n\nVisualizing data from Google Trends\ngoogle_trends_ch <- read_csv(here(\"blog\",\"2022-11-30-ArcGIS-vs-QGIS\", \"google-trends-CH.csv\"),skip = 1)\ngoogle_trends_world <- read_csv(here(\"blog\",\"2022-11-30-ArcGIS-vs-QGIS\", \"google-trends-World.csv\"),skip = 1)\n\ngoogle_trends_world$`QGIS: (Worldwide)` <- parse_number(google_trends_world$`QGIS: (Worldwide)`)\n\ngoogle_trends <- full_join(google_trends_ch, google_trends_world) |>  \n  pivot_longer(-Month) |> \n  extract(name, c(\"Software\", \"Region\"), regex = \"(\\\\w+):\\\\s\\\\((\\\\w+)\\\\)\") |> \n  mutate(Month = as.POSIXct(paste0(Month,\"-1\"), format = \"%Y-%m-%d\"))\n\ngoogle_trends_roll <- google_trends |> \n  group_by(Software, Region) |> \n  mutate(roll = zoo::rollmean(value, 12, align = \"left\", fill = NA)) |>\n  filter(!is.na(roll)) |>\n  ungroup() |>\n  complete(Software, Region, fill = list(value = 0, roll = 0))\n\n\n\n\nVisualizing data from Google Trends\nggplot(filter(google_trends_roll, Region == \"Switzerland\"), aes(Month, roll, color = Software)) +\n  with_blur(\n    geom_line(data = ~filter(., Software == \"ArcGIS\"), \n              color = cols[1], size = 2.5),\n    colour = lighten(cols[1], .1), sigma = 3\n  ) +\n  with_blur(\n    geom_line(data = ~filter(., Software == \"QGIS\"), \n              color = cols[2], size = 2.5),\n    colour = lighten(cols[2], .1), sigma = 3\n  ) +\n  annotate(\"text\", x = as.POSIXct(\"2019-10-01\"), y = 42,vjust = .5, hjust = .5,\n           label = \"Probably Corona Dashboard searches\",\n           family = \"Chivo\",\n           colour = cols[\"arcgis\"],\n           size = 3.8,\n           lineheight = .9) +\n  geom_segment(aes(x = as.POSIXct(\"2019-12-01\"), xend = as.POSIXct(\"2019-12-01\"), \n                   y = 40, yend = 35),colour = cols[\"arcgis\"]) +\n  theme_void() +\n  geom_richtext(\n    aes(x = as.POSIXct(\"2013-01-01\"), y = 55,\n        label = glue::glue(\"Google Trends for <span style='font-family:cinzel;'><b style='color:{cols['arcgis']};font-size:30pt;'>[ArcGIS]</b> & <b style='color:{cols['qgis']};font-size:30pt;'>[QGIS]</b></span> search terms in Switzerland\")),\n    color = \"grey80\", size = 7, lineheight = 1.5, family = \"Work Sans\",\n    stat = \"unique\", fill = NA, label.color = NA\n  ) +\n  labs(caption = \"Data from Google Trends | Visualisation by Nils Ratnaweera | ratnaweera.xyz\") +\n  theme(\n    panel.background = element_rect(fill = \"transparent\", color = \"transparent\"),\n    plot.background = element_rect(fill = bgcol),\n    axis.text = element_text(family = \"Cinzel\", color = \"grey80\",\n                               size = 10, face = \"bold\", margin = margin(t = 6)),\n    panel.grid.major = element_line(linetype = 2,colour = \"grey50\"),\n    plot.margin = margin(15, 30, 10, 30),\n    plot.caption = element_text(family = \"Work Sans\", color = \"grey50\", size = 8,\n                                hjust = .5, margin = margin(t = 30, b = 0)),\n    legend.position = \"none\"\n  )\n\n\n\n\n\nThis data range back further than the data from gis.stackexchange.com. But similarly, both terms seem to have gained in popularity since around 2010 and ArcGIS hits a plateau around 2015. QGIS continues to gain in popularity and nearly catches up with the number of ArcGIS queries at the end of 2019. With the beginning of the pandemic, the search term ArcGIS spikes. I’m guessing this has to do with searches related to the ArcGIS Covid dashboard. Interestingly, in the most recent years QGIS seems to have overtaken ArcGIS in the number of times it was searched on Google.\nQGIS is evidently very popular in Switzerland. The worldwide data shows very similar trends, but the offset between ArcGIS and QGIS is larger.\n\n\nCode\nggplot(filter(google_trends_roll, Region == \"Worldwide\"), aes(Month, roll, color = Software)) +\n  with_blur(\n    geom_line(data = ~filter(., Software == \"ArcGIS\"), \n              color = cols[1], size = 2.5),\n    colour = lighten(cols[1], .1), sigma = 3\n  ) +\n  with_blur(\n    geom_line(data = ~filter(., Software == \"QGIS\"), \n              color = cols[2], size = 2.5),\n    colour = lighten(cols[2], .1), sigma = 3\n  ) +\n  annotate(\"text\", x = as.POSIXct(\"2019-10-01\"), y = 60,vjust = .5, hjust = .5,\n           label = \"Probably Corona Dashboard searches\",\n           family = \"Chivo\",\n           colour = cols[\"arcgis\"],\n           size = 3.8,\n           lineheight = .9) +\n  geom_segment(aes(x = as.POSIXct(\"2019-12-01\"), xend = as.POSIXct(\"2019-12-01\"), \n                   y = 60, yend = 50),colour = cols[\"arcgis\"]) +\n  theme_void() +\n  geom_richtext(\n    aes(x = as.POSIXct(\"2013-01-01\"), y = 70,\n        label = glue::glue(\"Google Trends for <span style='font-family:cinzel;'><b style='color:{cols['arcgis']};font-size:30pt;'>[ArcGIS]</b> & <b style='color:{cols['qgis']};font-size:30pt;'>[QGIS]</b></span> search terms Worldwide\")),\n    color = \"grey80\", size = 7, lineheight = 1.5, family = \"Work Sans\",\n    stat = \"unique\", fill = NA, label.color = NA\n  ) +\n  labs(caption = \"Data from Google Trends | Visualisation by Nils Ratnaweera | ratnaweera.xyz\") +\n  theme(\n    panel.background = element_rect(fill = \"transparent\", color = \"transparent\"),\n    plot.background = element_rect(fill = bgcol),\n    axis.text = element_text(family = \"Cinzel\", color = \"grey80\",\n                               size = 10, face = \"bold\", margin = margin(t = 6)),\n    panel.grid.major = element_line(linetype = 2,colour = \"grey50\"),\n    plot.margin = margin(15, 30, 10, 30),\n    plot.caption = element_text(family = \"Work Sans\", color = \"grey50\", size = 8,\n                                hjust = .5, margin = margin(t = 30, b = 0)),\n    legend.position = \"none\"\n  )\n\n\n\n\n\nIt must be said that neither data source can accurately reflect the real-world popularity of ArcGIS or QGIS. For instance, they do not show many users or companies use the respective software or how much money it generates. Nonetheless, the data has convinced me even more that the time is right to bet on QGIS and exclusively use FOSS in the classroom preferably and whenever possible (edited due to valid feedback by @sfkeller).\nI would like to finish off by providing you with a very insightful talk given by David Abernathy at the NC GIS Conference 2017. He talks about his university’s transition from ArcGIS to FOSS and makes several highly interesting points. If you’ve made it up to here, please watch his video!\n\n\n\n\n  \nEdit (2022-12-07)\nAnd lastly, since someone had do do it, a poem of the battle between ArcGIS and QGIS, written by chatGPT from openai.com\n\nIn ArcGIS and QGIS, two titans of their trade, A battle rages, with maps and data as their blade, Each strives to outdo the other, to claim the victory, And prove their worth to all who seek the truth in geography.  ArcGIS, with its sleek design and user-friendly interface, Makes mapping and analysis a breeze, and leaves no trace, Of confusion or frustration, for its tools are intuitive, And its functions diverse, enabling users to create maps that are creative.  QGIS, on the other hand, is open-source and free, A favorite among those who seek to break the mold, Its capabilities are vast, and its potential immense, And with its support for numerous plugins, it can be tailored to fit any role.  But as the battle rages on, with each program giving its all, It becomes clear that there can be only one winner, after all, And in the end, the victor is not ArcGIS nor QGIS, But rather the one who uses their tools with skill and expertise.  So let us all embrace these powerful programs, And use them wisely, to unlock the mysteries of the land, For in the end, it is not the tool, but the user’s hand, That shapes the world, and reveals its hidden secrets for all to understand."
  },
  {
    "objectID": "blog/2021-12-01-opendata-per-canton/opendata-per-canton.html",
    "href": "blog/2021-12-01-opendata-per-canton/opendata-per-canton.html",
    "title": "How open are our cantons?",
    "section": "",
    "text": "Just a couple of years ago, we never used geodata from cantons when doing a Swiss wide project. The data was just too inhomogeneous, too hard to assemble and to expensive to acquire. Now, with geodienste.ch up and running, this task has become much much simpler. Or so I thought.\nTo be fair, the platform is amazing (although lacking an API) and most cantons are very accommodating, providing the data freely. Some cantons however, are still extremely restrictive with their publicly funded data, asking for close to 50’000 CHF (!!) for commercial use of their data! I’m lucky enough to be working in a research project at a university, but I can’t help but mourn the opportunities this data could be used for if it was provided freely.\nAnyway, this experience lead me to the question: Which cantons are most open in their data sharing policy? Which cantons are more restrictive? To answer this question, I scraped the title-page of each of the 23 datasets on geodienste.ch and visualized the results.\ngeodienste.ch provides three different methods to obtain the data. Either 1) the data is freely available without registration, 2) the data is freely available after registering on the website or 3) the canton needs to approve your request. This last method can mean that you will be grated approval within a few hours, or that you need to wait a couple of days to receive an email asking you to pay horrendous amounts (if you want to use it commercially).\nThe results show that most datasets available on the website are offered freely and without the need of registration. Some few datasets require registration, and only 6 cantons feel the need to manually approve and potentially charge certain datasets. Foremost, the cantons Jura, Ticino and Valais heavily guard their data and require approval on a large number of their datasets.\nMost cantons offer between 10 and 15 datasets on geodienste.ch. The canton Schwyz has the highest number of datasets online (20), while Zug, Bern and Schaffhausen share second place with 18 datasets each. All provide their data to unregistered users freely. Good for you, that is the way to go!\n\n\nshow code for webscraping\nlibrary(httr)\nlibrary(xml2)\nlibrary(rvest)\nlibrary(tidyverse)\n\nservices <- read_html(\"https://geodienste.ch/versions_overview\") %>%\n  html_elements(\"a\") %>%\n  html_attr(\"href\")\n\nservices <- services[str_detect(services, \"/services/\")]\n\n\nkantone <- read_html(\"https://geodienste.ch/services/av\") %>%\n  html_elements(\".wappen-kt\") %>%\n  html_text()\n\nkantone <- str_trim(kantone)\nwappen <- read_html(\"https://geodienste.ch/services/av\") %>%\n  html_elements(\".wappen-kt\") %>%\n  html_elements(\"img\") %>%\n  html_attr(\"src\")\n\nkantone2 <- paste0(kantone,str_extract(wappen, \"\\\\.\\\\w{3}$\"))\n\n\nmap2(kantone2, wappen, function(x,y){\n  download.file(paste0(\"https://geodienste.ch/\",y),file.path(\"wappen\",x))\n})\n\nfi <- list.files(\"wappen/\",\".png\", full.names = TRUE)\n\nfile.rename(fi, str_remove(fi, \" \"))\n\n\nmyres <- map(services, function(url_i){\n  res <- read_html(paste0(\"https://geodienste.ch/\",url_i)) %>%\n    # html_elements(\".canton-table\") %>%\n    html_table() %>%\n    magrittr::extract2(1)\n  \n  res <- res %>% \n    janitor::clean_names()\n  \n  res %>%\n    filter(!is.na(info))\n})\n\nres2 <- map2_dfr(myres, services, function(mydf, serv){\n  mydf <- mydf[,1:3]\n  mydf$services <- serv\n  mydf\n})\n\nwrite_csv(res2, \"geodienste-raw.csv\")\n\n\n\n\nshow code for data preparation\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(cowplot)\n\n\nres2 <- read_csv(\"geodienste-raw.csv\")\nfacs <- c(\"Frei erhältlich\",\"Registrierung erforderlich\",\"Freigabe erforderlich\",\"Im Aufbau\", \"keine Daten\")\n\nres3 <- res2 %>%\n  separate(verfugbarkeit, c(\"verfugbarkeit\",\"verfugbarkeit2\"), sep = \"\\\\n\\\\s+\") %>% \n  select(-verfugbarkeit2) %>%# verfugbarkeit2 seems to be erroneous \n  mutate(\n    erhaeltlich_ab = str_extract(verfugbarkeit, \"\\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{4}\"),\n    verfugbarkeit = str_remove(verfugbarkeit, \"\\\\s\\\\(.+\\\\)\")\n  ) %>%\n  rename(kanton = x)\n\n\nres3_wide <-res3 %>%\n  group_by(kanton, verfugbarkeit) %>%  \n  count() %>%\n  mutate(verfugbarkeit_code = paste0(\"verf\",as.integer(factor(verfugbarkeit, facs)))) %>%\n  ungroup() %>%\n  select(-verfugbarkeit) %>%\n  pivot_wider(names_from = verfugbarkeit_code, values_from = n,values_fill = 0) %>%\n  select(kanton, order(colnames(.))) %>%\n  arrange(across(starts_with(\"verf\")))\n\n\nres4 <- res3 %>%\n  mutate(\n    kanton = factor(kanton, levels = res3_wide$kanton, ordered = TRUE),\n    verfugbarkeit = factor(verfugbarkeit, levels = facs, ordered = TRUE),\n    x = 1\n  ) \n\n\nwappen_df <- tibble(file = list.files(\"wappen\",full.names = TRUE)) %>%\n  mutate(\n    kanton = str_extract(file, \"[A-Z][A-Z]\"),\n) %>%\n  left_join(res3_wide, ., by = \"kanton\") %>%\n  mutate(y = row_number())\n\n\n\n\nshow code for creating the plot\ncols <- rev(RColorBrewer::brewer.pal(5, \"RdYlGn\"))\n\nplot_bg_col <- Sys.getenv(\"plot_bg_col\") \ntext_col <- Sys.getenv(\"text_col\")\n\np <- res4 %>%\n  ggplot(aes(x, kanton, fill = verfugbarkeit)) + \n  geom_col(position = position_stack(reverse = TRUE), color = plot_bg_col) +\n  pmap(select(wappen_df, file, y), function(file, y){draw_image(file, x = -0, y = y, width = 0.8, height = 0.6,hjust = 1,vjust = 0.5)}) +\n  scale_fill_manual(\"Status\",values = cols) +\n  scale_x_continuous(\"Anzahl Datensätze\",sec.axis = sec_axis(~./23,\"Anteil der Datensätze\", labels = scales::percent_format())) +\n  # guides(fill = guide_legend(title.position = \"top\",title.hjust = 0.5)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        axis.title.y = element_blank(), \n        panel.grid = element_blank(),\n        plot.background = element_rect(fill = plot_bg_col, colour = NA),\n        panel.background = element_rect(fill = plot_bg_col, colour = NA),\n        legend.text = element_text(size = 7),\n        legend.title = element_blank(),\n        text = element_text(colour = text_col),\n        axis.text = element_text(colour = text_col)\n        ) +\n  coord_equal() #+ theme(plot.background = element_rect(fill = \"plot_bg_col\"))\n\nggsave(\"preview.png\", width = 15, height = 10, units = \"cm\",scale = 1.4)"
  },
  {
    "objectID": "blog/2021-08-13-minimalistic-topography/minimalistic-topography.html",
    "href": "blog/2021-08-13-minimalistic-topography/minimalistic-topography.html",
    "title": "Minimalistic topography",
    "section": "",
    "text": "“So beautiful that it hurts”1 Bauhasaurus wrote in his tweet, posting an image by Carla Martínez Sastre. The artist had used a beautiful, clever and minimalistic way to visualize the topography of South America.\nThe way I understand it, Carla drew “horizontal” (latitudanal) elevation profiles at equal intervals over the continent and filled these elevation profiles to visualize not only the continent’s topography, but also implicitly showing it’s borders.\nI found this a very nice approach and tried recreating this idea with R for my home country, Switzerland. I’m quite happy with the result, however there is still a lot of room for improvement. I’ve packed the approach into generic functions, see below for the complete source code. Check below to see the source code.\n\n\nCreate some generic functions\n\n#' Create ridgelines from a digital elevation model (dhm)\n#'\n#' dhm: path to a dhm that can be imported using terra::rast\n#' n_lines: how many lines / polygons do you want to draw? Default is 50\n#' vspace: vertical space between lines, in units provided by the dhm. This overrides n_lines\n#' fac: How much of the space between the lines should be occupied by the hightest elevation?\n#' point_density: Density of the point samples used to extract elevation. Defaults to the inverse of the raster resolution\n#' geom_type: What should the output geometry type be? Can be LINESTRING or POLYGON\ncreate_ridges <- function(dhm, n_lines = 50, vspace = NULL, fac = 2, point_density = NULL, geom_type = \"LINESTRING\"){\n  \n  library(sf)\n  library(terra)\n  library(purrr)\n  \n  # extract the extent of the dhm as a vector\n  ex <- ext(dhm) %>%\n    as.vector()\n  \n  # If vspace is NULL (default), then vspace is calculated using n_lines\n  if(is.null(vspace)){\n    vspace <- (ex[\"ymax\"] - ex[\"ymin\"])/n_lines\n  }\n  \n  \n  point_density <- if(is.null(point_density)){1/terra::res(dhm)[2]}\n  \n  # Defines at what y-coordinates elevation should be extracted\n  heights <- seq(ex[\"ymin\"], ex[\"ymax\"], vspace)\n  \n  # calculates the x/y coordinates to extract points from the dhm\n  mypoints_mat <- map(heights, function(height){\n    matrix(c(ex[\"xmin\"], height, ex[\"xmax\"], height), ncol = 2, byrow = TRUE) %>%\n      st_linestring()\n  }) %>%\n    st_as_sfc() %>%\n    st_line_sample(density = point_density,type = \"regular\") %>%\n    st_as_sf() %>%\n    st_cast(\"POINT\") %>%\n    st_coordinates()\n  \n  \n  # extracts the elevation from the dhm\n  extracted <- terra::extract(dhm, mypoints_mat) %>% \n    cbind(mypoints_mat) %>% \n    as_tibble()\n  \n  # calculates the factor with which to multiply elevation, based on \"fac\" and the maximum elevation value\n  fac <- vspace*fac/max(extracted[,1], na.rm = TRUE)\n  \n  # calculates the coordinats of the ridge lines\n  coords <-extracted %>%\n    filter(!is.na(extracted[,1])) %>%\n    split(.$Y) %>%\n    imap(function(df, hig){\n      hig <- as.numeric(hig)\n      Y_new <- hig+pull(df[,1])*fac\n      matrix(c(df$X, Y_new), ncol = 2)\n    })\n\n  # creates LINESTRING or POLYGON, based on the \"geom_type\"\n  geoms <- if(geom_type == \"LINESTRING\"){\n    map(coords, ~st_linestring(.x))\n  } else if(geom_type == \"POLYGON\"){\n    imap(coords, function(x, hig){\n      hig <- as.numeric(hig)\n      \n      first <- head(x, 1)\n      first[,2] <- hig\n      last <- tail(x, 1)\n      last[,2] <- hig\n      \n      st_polygon(list(rbind(first, x, last, first)))\n    })\n  } else{\n    stop(paste0(\"This geom_type is not implemented:\",geom_type,\". geom_type must be 'LINESTRING' or 'POLYGON'\"))\n  }\n  \n  # adds the CRS to the output sfc\n  dhm_crs <- crs(dhm)\n  \n  if(dhm_crs == \"\") warning(\"dhm does not seem to have a CRS, therefore the output does not have a CRS assigned either.\")\n  \n  geoms %>%\n    st_sfc() %>%\n    st_set_crs(dhm_crs)\n  \n}\n\n# A helper function to creteate a polygon from the extent of a (dhm) raster\nst_bbox_rast <- function(rast_obj){\n  \n  library(terra)\n  library(sf)\n  \n  ex <- ext(rast_obj) %>%\n    as.vector()\n  \n  matrix(c(ex[1],ex[3],ex[1], ex[4],ex[2], ex[4],ex[2],ex[3],ex[1],ex[3]),ncol = 2, byrow = TRUE) %>%\n  list() %>%\n  st_polygon() %>% \n    st_sfc(crs = crs(rast_obj))\n}\n\n\n\nImport data and use the functions\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\n# library(ragg)\n\n\ndhm <- terra::rast(\"data-git-lfs/DHM25/DHM200.asc\")\ncrs(dhm) <- \"epsg:21781\"\n\n\nswitzerland_21781 <- sf::read_sf(\"data-git-lfs/swissboundaries/swissBOUNDARIES3D_1_3_TLM_LANDESGEBIET.shp\") %>%\n  st_union() %>%\n  st_transform(21781) \n\nmymask <- st_bbox_rast(dhm) %>%\n  st_buffer(5000) %>%\n  st_difference(switzerland_21781)\n\n\n\n\n\nsf_obj <- create_ridges(dhm,n_lines = 35, fac = 1.1,geom_type = \"POLYGON\")\n\n# bg_color <- \"#27363B\"\nbg_color <- Sys.getenv(\"plot_bg_col\")\nfg_color <- \"#EB4960\"\nfamily <- \"FreeMono\"\n\n\n\n\nbbox_switz <- st_bbox(switzerland_21781)\nbbox_switz_enlarge <- st_buffer(st_as_sfc(bbox_switz),50000)\nlims <- st_bbox(bbox_switz_enlarge)\nxlims =  lims[c(\"xmin\",\"xmax\")]\nylims = lims[c(\"ymin\",\"ymax\")]\n\nasp <- diff(ylims)/diff(xlims)\n\n\n\n\nCode\nmyplot <- ggplot(sf_obj) +\n  geom_sf(color = \"NA\", fill = fg_color)  + \n  geom_sf(data = mymask, color = \"NA\", fill = bg_color) +\n  # geom_sf(data = bbox_switz_enlarge, fill = \"NA\") +\n  ggtext::geom_richtext(aes(x = median(xlims), y = quantile(ylims,0.95), label = \"Topography of Switzerland\"), family = family, fill = NA, label.color = NA, hjust = 0.5, size = 6, color = fg_color)+\n  ggtext::geom_richtext(aes(x = median(xlims), y = ylims[\"ymin\"], label = \"Data from ©swisstopo<br>visualized by Nils Ratnaweera\"), family = family, fill = NA, label.color = NA, hjust = 0.5, size = 3.5, color = fg_color)+\n  theme_void() +\n  theme(plot.background = element_rect(fill = bg_color,color = NA)) +\n  coord_sf(datum = 21781,xlim =  xlims, ylim = ylims);\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nOriginal (Esp): “Tan linda que duele.”↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Ratnaweera-XYZ",
    "section": "",
    "text": "Teaching geospatial: ArcGIS or QGIS?\n\n\n\n\n\nEvery year we teach well over a hundred Bachelor and Master students basic and advanced concepts…\n\n\n\n\n\n\nNov 20, 2022\n\n\nNils Ratnaweera\n\n\n\n\n\n\n  \n\n\n\n\nHow open are our cantons?\n\n\n\n\n\nFederalism at its best: How the open data policy is handled across cantons\n\n\n\n\n\n\nDec 1, 2021\n\n\nNils Ratnaweera\n\n\n\n\n\n\n  \n\n\n\n\nWarping Switzerland back into shape\n\n\n\n\n\nWhat changed when switching from Switzerland’s old coordinate system to the new one?\n\n\n\n\n\n\nNov 5, 2021\n\n\nNils Ratnaweera\n\n\n\n\n\n\n  \n\n\n\n\nBenchmarking binary predicates\n\n\n\n\n\nComparing the performance of different methods to do a “point in polygon operation” with sf.\n\n\n\n\n\n\nOct 22, 2021\n\n\nNils Ratnaweera\n\n\n\n\n\n\n  \n\n\n\n\nMinimalistic topography\n\n\n\n\n\nA beautiful way to visualize topography, inspired by Carla Martínez Sastre\n\n\n\n\n\n\nAug 13, 2021\n\n\nNils Ratnaweera\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching Materials",
    "section": "",
    "text": "Most of our teaching materials are available online on github repos. Check them out!\n\n\n\n\n\n\n\nAn introduction to Spatial R for ArcGIS users  arc2r.github.io/book\n\n\n\n\n\n\nAn introduction to GIS in Python for Environmental Scientists (Bachelor Students)  modul-agi.github.io\n\n\n\n\n\n\n\nTeaching R, the tidyverse, data processing, data visualization and statistics  researchmethods-zhaw.github.io\n\n\n\n\n\n\n\nComputational Movement Analysis: Detecting Patterns and Trends in Environmental Data. A course taught by Prof. Dr. Patrick Laube, I was responsible for creating the R-exercises.  computationalmovementanalysis.github.io"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Ratnaweera-XYZ",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "apps/geodienste-kantone/index.html",
    "href": "apps/geodienste-kantone/index.html",
    "title": "How Open Are Our Cantons?",
    "section": "",
    "text": "Geodienste.ch is an excellent website to get cantonal geodata. The harmonization process not only simplifies the acquisition of the data, it also shows which cantons are not as open with their data as they could be.\n\nhtml`<p>This visualization is a *live updating* version of [a static chart](blog/2021-12-01-opendata-per-canton/opendata-per-canton.html) I created quite a while ago. I am particularly proud of the live updating functionality. The data you are seeing below is from the very minute you've visited this site, i.e.: <span style=\"background-color: black; padding: 3px;    font-family: mono; font-size:small; border-radius: 3px\">${new Date(now).toString()}<span></p>`\n\n\n\n\n\n\n\n\n\n\n\n\nd3 = require('d3')\n\n\n\n\n\n\n\ngeodienste = await d3.json(\"https://www.geodienste.ch/info/services.json?base_topics=av%2Cfixpunkte%2Cfruchtfolgeflaechen%2Cgefahrenkarten%2Ckantonale_ausnahmetransportrouten%2Ckataster_belasteter_standorte%2Clwb_bewirtschaftungseinheit%2Clwb_biodiversitaetsfoerderflaechen%2Clwb_elemente_mit_landschaftsqualitaet%2Clwb_nutzungsflaechen%2Clwb_perimeter_ln_sf%2Clwb_perimeter_terrassenreben%2Clwb_rebbaukataster%2Cleitungskataster%2Cnpl_laermempfindlichkeitsstufen%2Cnpl_nutzungsplanung%2Cplanerischer_gewaesserschutz%2Crevitalisierung_seen%2Cplanungszonen%2Cnpl_waldgrenzen%2Cstromversorgungssicherheit_netzgebiete%2Cnpl_waldabstandslinien%2Cwaldreservate%2Cwildruhezonen&cantons=AG%2CAI%2CAR%2CBE%2CBL%2CBS%2CFR%2CGE%2CGL%2CGR%2CJU%2CLU%2CNE%2CNW%2COW%2CSG%2CSH%2CSO%2CSZ%2CTG%2CTI%2CUR%2CVD%2CVS%2CZG%2CZH&restricted_topics=true&language=de\")\n//ignored the following datasets (since there is not data on these for most cantons)\n//luftbild\n//naturereigniskataster\n//richtplanung_erneuerbare_energien\n\n//the folowing character string determins a break: %2C\n\n\n\n\n\n\n\n// sort the cantons by the most amount of datasets of type \"frei erhältlich\", then by Registrierung erforderlich and so on\n\ngeodienste_canton_sort = {\n\n  let aggregated = Array()\n\n  let idx = -1\n  geodienste.services.forEach(i => {\n    idx +=1\n    const canton = i.canton\n    const publication_data = i.publication_data\n    const idx_canton = aggregated.map(x  => x.canton).indexOf(canton)\n    let score\n    if (publication_data == \"Frei erhältlich\") {\n      score = 1000000\n    } else if(publication_data == \"Registrierung erforderlich\"){\n      score = 10000\n    } else if(publication_data == \"Freigabe erforderlich\"){\n      score = 100\n    } else{\n      score = 1\n    }\n      \n    if(idx_canton == -1){\n\n      aggregated[idx] = {canton: canton, score: score}\n     \n    } else{\n      aggregated[idx_canton][\"score\"] += score \n      \n    }\n  \n  });\n\n  return aggregated\n}\n\n\n\n\n\n\n\n// from this sorted array, only extract the order\nkantone_order = geodienste_canton_sort.sort((a,b) => b.score - a.score).map(x => x.canton)\n\n\n\n\n\n\n\n// group the array by canton\ngroupedArray = geodienste.services.reduce((groups, item) => {\n  const group = groups.find(g => g.canton === item.canton);\n  if (!group) {\n    groups.push({\n      canton: item.canton,\n      items: [item],\n    });\n  } else {\n    group.items.push(item);\n  }\n  return groups;\n}, []);\n\n\n\n\n\n\n\n// manually define the order of the datasets\npublication_data_order = [\"Frei erhältlich\", \"Registrierung erforderlich\", \"Freigabe erforderlich\", \"Im Aufbau\", \"keine Daten\"]\n\n\n\n\n\n\n\n// sort the array within each canton by the publication data order\nsortedArray = groupedArray.map(group => {\n  return (group.items.sort((a, b) => publication_data_order.indexOf(a.publication_data) - publication_data_order.indexOf(b.publication_data)))\n});\n\n\n\n\n\n\n\n// flatten the result again (ungroup)\nflattedArray = sortedArray.flat()\n\n\n\n\n\n\n\ndateTime = {\n  let current = new Date();\n  let cDate = current.getFullYear() + '-' + (current.getMonth() + 1) + '-' + current.getDate()\n  let cTime = current.getHours() + \":\" + current.getMinutes() + \":\" + current.getSeconds()\n  return(cDate + ' ' + cTime)\n}\n\n\n\n\n\n\n\nAvailability of Cantonal Geodata on Geodienste.ch\n\nPlot.plot({\n  caption: html`<div>Live data from geodienste.ch restAPI, last update: ${dateTime}<br>Visualized by Nils Ratnaweera</div>`,\n  width: width,\n  height: width,\n  y: {\n    domain: kantone_order,\n    label: \"Kanton →\",\n     },\n  x: {\n    label: \"Anzahl Datensätze →\"\n  },\n  color: {\n    range: ['#2b83ba','#abdda4','#ffffbf','#fdae61','#d7191c'],\n    legend: \"swatches\",\n    label: \"Status\",\n    domain: publication_data_order\n  },\n  style: {\n    backgroundColor: \"#073b4c\",\n    color: \"#ADB5BD\"\n  },\n  marks: [\n    Plot.barX(flattedArray, {\n      x: 1, \n      y: \"canton\", \n      fill: \"publication_data\",\n      stroke: \"#073b4c\", \n      strokeWidth: 2\n    }),\n  ]\n})"
  },
  {
    "objectID": "Readme.html",
    "href": "Readme.html",
    "title": "Ratnaweera-XYZ",
    "section": "",
    "text": "This site has github pages activated via the gh-pages branch, and a CNAME entry pointing to www.ratnaweera.xyz (note the www.). To publish the website, I need to push the content of the _site/ folder to the gh-pages branch. This is automated with the quarto publish tool using the following command:\nquarto publish gh-pages --no-prompt\nAt my provider, I’ve created four A-Records pointing to 185.199.108.153-185.199.111.153 (see these instructions). This redirects EVERYTHING from ratnanil.github.io to ratnaweera.xyz (including pages of other repos that I’m the owner of). I’ve added an additional CNAME record for the www subdomain (www.ratnaweera.xyz, see here).\nTo make sure that no domains can be taken over, I’ve verified my domains on github using these instructions.\nPS: Unlike before, I completely omit using netlify, since this just creates additional overhead without much gain."
  },
  {
    "objectID": "Readme.html#preview-images-twitter-cards",
    "href": "Readme.html#preview-images-twitter-cards",
    "title": "Ratnaweera-XYZ",
    "section": "Preview images / Twitter cards",
    "text": "Preview images / Twitter cards\nPreview images are used in the blog listing, but also as twitter cards (preview image when tweeting a like to a specific site)\n\nAll preview images should have an aspect ratio of 3:2 (width:height). When working with ggplot2, 15cm x 10cm is usually fine.\nThey have a background color of “#073b4c”\nText color: #ADB5BD\nTo publish a preview image to gh-pages, you need to add it in resources. You can then reference the full, relative path:\n\n---\nimage: /apps/geodienste-kantone/preview.png\nresources: preview.png\n---"
  },
  {
    "objectID": "Readme.html#large-files",
    "href": "Readme.html#large-files",
    "title": "Ratnaweera-XYZ",
    "section": "Large Files",
    "text": "Large Files\nI’ve had too many repos that have become huge because I thoughlessly committed large files. Using git lfs as a workaround was not very satisfactory either, since it’s not easy to free up the quota of git lsf my simply deleting files from the history (you have to delete the whole repo intead!). I’m still looking into this, I think good practice for now would be to simply put all large files in subdirectories containing a .gitingore file containing a *."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ratnaweera XYZ",
    "section": "",
    "text": "Hi there 👋\n\n\n\n\n\nMy name is Nils Ratnaweera and I’m a Research Fellow at the Zurich University of Applied Sciences. I love my Data Scientist job of working with environmental data using mainly open source tools such as R, python, gdal, ogr2ogr, PostgresSQL, PostGIS.\n\n\n\n\n\nUPDATE: I’m interested in collaborating on projects in and also outside my (part-time) day job. If you have a project that you think I can help with, please get in touch. I’m also available for short-term contract work. You can email me or directly book a slot 📆 here\n\nYou can find me on GitHub, Twitter, Mastodon and StackOverflow. Apropos StackOverflow, here is a live updating visualization of my Top Tags:\n\nMy Top Tags on stackoverflow.com and gis.stackexchange.com\n\ndateTime = {\n  let current = new Date();\n  let cDate = current.getFullYear() + '-' + (current.getMonth() + 1) + '-' + current.getDate()\n  let cTime = current.getHours() + \":\" + current.getMinutes() + \":\" + current.getSeconds()\n  return(cDate + ' ' + cTime)\n}\n\n\n\n\n\n\n\nmd`(live data from ${dateTime}, acquired with the [stackexchange API](https://api.stackexchange.com), visualized with *Observable Plot*)`\n\n\n\n\n\n\n\nd3 = require(\"d3\")\n\n\n\n\n\n\n\nstackoverflow = await d3.json(\"https://api.stackexchange.com/2.3/users/4139249/top-tags?site=stackoverflow\")\n//stackoverflow = FileAttachment(\"top-tags-stackoverflow.json\").json()\n\n\n\n\n\n\n\n//gis = FileAttachment(\"top-tags-gis-stackexchange.json\").json()\ngis = await d3.json(\"https://api.stackexchange.com/2.3/users/40929/top-tags?site=gis.stackexchange.com\")\n\n\n\n\n\n\n\ngis2 = gis.items.map(function(x){\n  return [\n    {source: \"gis.stackexchange.com\", type: \"Answer\", tag: x.tag_name, score: x.answer_score},\n    {source: \"gis.stackexchange.com\", type: \"Question\", tag: x.tag_name, score: x.question_score}\n  ]\n}).flat()\n\n\n\n\n\n\n\nstackoverflow2 = stackoverflow.items.map(function(x){\n  return [\n    {source: \"stackoverflow.com\", type: \"Answer\", tag: x.tag_name, score: x.answer_score},\n    {source: \"stackoverflow.com\", type: \"Question\", tag: x.tag_name, score: x.question_score}\n  ]\n}).flat()\n\n\n\n\n\n\n\nstackexchange = gis2.concat(stackoverflow2)\n\n\n\n\n\n\n\nPlot.plot({\n  marginLeft: 100,\n  caption: html`<div>Live data from the stackexchange restAPI, last update: ${dateTime}<br>Visualized by Nils Ratnaweera</div>`,\n  width: width,\n  y: {\n    label: \"\",\n  },\n  x: {\n    label: \"Score\"\n  },\n  color:{\n    legend: true\n  },\n  domain: {\n    color: [\"question_score\",\"answer_score\"]\n  },\n  marks: [\n    Plot.barX(stackexchange, \n    Plot.groupY({x: \"sum\"},{\n      y: \"tag\", \n      x: \"score\", \n      fill: \"type\",\n      sort: {y: \"x\", reverse: true},\n      filter: (i) => i.score > 8\n      })\n      ),\n  ],\n  style: {\n    backgroundColor: \"#073b4c\",\n    color: \"#ADB5BD\"\n  }\n})"
  }
]